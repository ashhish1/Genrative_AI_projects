Project 1: MCP-powered Agentic RAG
Overview
In this project, you'll build a local agentic RAG system using the Model Context Protocol (MCP) to modularly connect an LLM to external tools like web search, vector DBs, or file loaders. The system retrieves relevant context, injects it into prompts, and lets the agent reason autonomously.

Tech Stack
ComponentTool/LibLanguage ModelOllama (e.g., mistral, llama3)Agent Frameworkmcp Python library (local server)RAG PipelineLangChain or raw orchestrationVector StoreChromaDB (local, open-source)File HandlingPDF/Text LoaderFrontend (optional)Streamlit or CLIEnvironmentPython 3.10+, virtualenv or Conda

Project Structure
agentic-rag-mcp/
├── main.py
├── mcp_config.yaml
├── vector_store/
├── data/
│   └── sample_docs/
├── tools/
│   └── chromadb_tool.py
├── rag_agent.py
└── requirements.txt
Step-by-Step Implementation
Step 1: Set up your local environment
python -m venv .venv
source .venv/bin/activate
pip install -U pip
pip install mcp ollama langchain chromadb pypdf
Start the Ollama server and pull a model:

ollama serve
ollama pull mistral  # or llama3
Step 2: Launch a local MCP Server
Create a file: main.py

from mcp import Agent, ToolRegistry
from tools.chromadb_tool import ChromaTool
from rag_agent import get_rag_response
 
# Register your tool (vector search tool for RAG)
tools = ToolRegistry()
tools.register(ChromaTool())
 
agent = Agent(
    name="rag-agent",
    llm="http://localhost:11434/api/generate",  # Ollama endpoint
    tools=tools,
    max_tokens=1024,
    temperature=0.7,
)
 
if __name__ == "__main__":
    agent.serve(port=5000)
Step 3: Define a Vector Search Tool
tools/chromadb_tool.py

from mcp.tools import Tool
import chromadb
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
 
class ChromaTool(Tool):
    def __init__(self):
        super().__init__(name="document_search", description="Search over documents")
        self.client = chromadb.Client()
        self.embedding_func = SentenceTransformerEmbeddingFunction(model_name="all-MiniLM-L6-v2")
        self.collection = self.client.create_collection(name="docs", embedding_function=self.embedding_func)
 
    def invoke(self, input_text: str) -> str:
        results = self.collection.query(query_texts=[input_text], n_results=3)
        return "\n\n".join([doc for doc in results["documents"][0]])
Step 4: Load Docs into Chroma Vector DB
from chromadb import Client
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
 
embedding_func = SentenceTransformerEmbeddingFunction(model_name="all-MiniLM-L6-v2")
client = Client()
collection = client.create_collection(name="docs", embedding_function=embedding_func)
 
# Load sample text or PDF
docs = ["Artificial Intelligence is revolutionizing many industries...", "MCP enables modular tool use for AI agents..."]
collection.add(documents=docs, ids=["doc1", "doc2"])
Step 5: Agent Query Logic (Optional)
rag_agent.py

def get_rag_response(query: str) -> str:
    import requests
    response = requests.post("http://localhost:5000/ask", json={"query": query})
    return response.json()["response"]
Running Your MCP-Powered RAG Agent
python main.py
Then from another terminal or via a frontend:

from rag_agent import get_rag_response
print(get_rag_response("What is the purpose of the MCP protocol?"))
Output
The response should combine retrieved documents from Chroma and a reasoned summary or answer generated by the LLM (Mistral, LLaMA, etc.) through the MCP server.

Optional Extensions
Add Streamlit frontend

Add PDF/text drag & drop upload for ingestion

Integrate Web Search Tool for fallback queries

Add agent memory (JSON store)
